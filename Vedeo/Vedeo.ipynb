{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66934f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/divs/Documents/Vedeo\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,Activation,Flatten,MaxPool2D,Dropout,BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(\"//home//divs//Documents//Vedeo//\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f1d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"sign_mnist_train.csv\")\n",
    "df_test=pd.read_csv(\"sign_mnist_test.csv\")\n",
    "df1=pd.DataFrame(df_train)\n",
    "df2=pd.DataFrame(df_test)\n",
    "\n",
    "y_train=df1.values[:,:1]\n",
    "y_test=df2.values[:,:1]\n",
    "x_train=df1.values[:,1:]\n",
    "x_test=df2.values[:,1:]\n",
    "\n",
    "x_train_new=[]\n",
    "x_test_new=[]\n",
    "\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    image=x_train[i]\n",
    "    \n",
    "    image=image.reshape(28,28)\n",
    "    x_train_new.append(image)\n",
    "   \n",
    "for i in range(len(x_test)):\n",
    "    image=x_test[i]\n",
    "    \n",
    "    image=image.reshape(28,28)\n",
    "    \n",
    "    x_test_new.append(image)\n",
    "    \n",
    "num_classes=26\n",
    "y_train_new=np.array(y_train).reshape(-1)\n",
    "y_test_new=np.array(y_test).reshape(-1)\n",
    "y_train_new=np.eye(num_classes)[y_train_new]\n",
    "y_test_new=np.eye(num_classes)[y_test_new]\n",
    "x_test_new=np.array(x_test_new)\n",
    "x_train_new=np.array(x_train_new)\n",
    "\n",
    "# print(x_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20fc19c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 28, 28)\n",
      "(27455, 26)\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 26) and (None, 1152) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m accuracy\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mevaluate(x\u001b[38;5;241m=\u001b[39mx_test_new,y\u001b[38;5;241m=\u001b[39my_test_new,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filey4xeo0mc.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/divs/.local/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 26) and (None, 1152) are incompatible\n"
     ]
    }
   ],
   "source": [
    "print(x_train_new.shape)\n",
    "print(y_train_new.shape)\n",
    "    \n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=8,kernel_size=(3,3),strides=(1,1),padding='same',input_shape=(28,28,1),activation='sigmoid',data_format='channels_last'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Conv2D(filters=8,kernel_size=(3,3),strides=(1,1),padding='same',activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool2D(pool_size=(4,4)))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train_new,y_train_new,epochs=25,batch_size=100)\n",
    "accuracy=model.evaluate(x=x_test_new,y=y_test_new,batch_size=100)\n",
    "print(accuracy[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f72450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 3, 3, 8)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3, 3, 128)         1152      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 26)                29978     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,794\n",
      "Trainable params: 31,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27576417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a357ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "[ WARN:0@37.978] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@37.979] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m hands \u001b[38;5;241m=\u001b[39m mp_hands\u001b[38;5;241m.\u001b[39mHands()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Convert the frame to RGB\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m rgb_frame \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Detect hands using Mediapipe\u001b[39;00m\n\u001b[1;32m     20\u001b[0m results \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(rgb_frame)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "labels = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# Initialize webca\n",
    "\n",
    "\n",
    "# Initialize Mediapipe hand detection\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret,frame = cap.read()\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands()\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect hands using Mediapipe\n",
    "    results = hands.process(rgb_frame)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        # Get the bounding box of the hand\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            x_min, x_max, y_min, y_max = 1000, 0, 1000, 0\n",
    "            for landmark in landmarks.landmark:\n",
    "                x, y = int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "\n",
    "            # Extract hand region\n",
    "            hand_frame = frame[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "            if hand_frame.shape[0] > 0 and hand_frame.shape[1] > 0:\n",
    "                # Preprocess the hand frame\n",
    "                resized_hand_frame = cv2.resize(hand_frame, (28, 28))\n",
    "                grayscale_hand_frame = cv2.cvtColor(resized_hand_frame, cv2.COLOR_BGR2GRAY)\n",
    "                flattened_hand_frame = grayscale_hand_frame.flatten()\n",
    "                normalized_hand_frame = flattened_hand_frame / 255.0\n",
    "\n",
    "                # Make a prediction\n",
    "                input_frame = normalized_hand_frame.reshape(1, 784)\n",
    "                prediction = model.predict(input_frame)\n",
    "                predicted_label = np.argmax(prediction)\n",
    "\n",
    "                # Get the corresponding letter\n",
    "                letter = labels[predicted_label]\n",
    "\n",
    "                # Display the prediction on the hand frame\n",
    "                cv2.putText(hand_frame, letter, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                # Display both frames in separate windows\n",
    "                cv2.imshow('Original Frame', frame)\n",
    "                cv2.imshow('Hand Frame', hand_frame)\n",
    "\n",
    "    # Exit when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9657f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "  \n",
    "  \n",
    "# define a video capture object\n",
    "\n",
    "while(True):\n",
    "      \n",
    "    # Capture the video frame\n",
    "    # by frame\n",
    "    vid = cv2.VideoCapture(0)\n",
    "  \n",
    "    ret, frame = vid.read()\n",
    "  \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "      \n",
    "    # the 'q' button is set as the\n",
    "    # quitting button you may use any\n",
    "    # desired button of your choice\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "  \n",
    "# After the loop release the cap object\n",
    "vid.release()\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3cd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649509ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4edab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18fa6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b567ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
